{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/Users/marilynwaldman/doggievae/notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch \n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "from src.mycategorical import CategoricalFeatures\n",
    "from src.dataset import CustomDataset\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds\n",
    "\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read csv into dataframe and create custom data set\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/ex4_2.csv\")\n",
    "\n",
    "# keep the length\n",
    "df_len = len(df)\n",
    "\n",
    "# Create a fake target column to \"fake\" dataset requirements.  This may not be needed.\n",
    "\n",
    "df[\"target\"] = -1\n",
    "\n",
    "# Slice off the target column to make a numpy array of targets, then delete the column\n",
    "\n",
    "targets_df =  df[\"target\"]\n",
    "del df['target']\n",
    "full_data = df\n",
    "\n",
    "# Create numpy arrays for data and targets, then create the CustomDataset\n",
    "\n",
    "data = df.to_numpy()\n",
    "targets = targets_df.to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "500\ntorch.Size([12])\n"
     ]
    }
   ],
   "source": [
    "# Create numpy arrays for data and targets, then create the CustomDataset\n",
    "data = df.to_numpy()\n",
    "targets = targets_df.to_numpy()\n",
    "\n",
    "model_dataset = CustomDataset(x = data, y = targets)\n",
    "total_count = len(model_dataset)\n",
    "print(total_count)\n",
    "print(model_dataset[0]['x'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'x': tensor([0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.]), 'y': tensor(-1)}\n"
     ]
    }
   ],
   "source": [
    "print(model_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_count = int(0.8 * total_count) \n",
    "valid_count = int(0.1 * total_count)\n",
    "test_count = total_count - train_count - valid_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'x': tensor([1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.]), 'y': tensor(-1)}\n"
     ]
    }
   ],
   "source": [
    "train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(model_dataset, (train_count, valid_count, test_count))\n",
    "\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "NUM_WORKER = 1\n",
    "\n",
    "#train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(model_dataset, (train_count, valid_count, test_count))\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKER)  \n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKER) \n",
    "test_loader  = torch.utils.data.DataLoader(test_dataset , batch_size=BATCH_SIZE, shuffle=False,num_workers=NUM_WORKER)\n",
    "dataloaders = {'train': train_loader, 'val': valid_loader, 'test': test_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'x': tensor([[0., 0., 0.,  ..., 1., 1., 0.],\n        [0., 1., 1.,  ..., 0., 1., 1.],\n        [1., 1., 1.,  ..., 0., 0., 0.],\n        ...,\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]]), 'y': tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1])}\n"
     ]
    }
   ],
   "source": [
    "# Create the data loader\n",
    "for data in train_loader:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x\ny\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the dataloader\n",
    "\n",
    "for data, target in test_loader:\n",
    "    print(data)\n",
    "    print(target)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the device\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model\n",
    "\n",
    "d = 2\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(12, d ** 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d ** 2, d * 2)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(d, d ** 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d ** 2, 12),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def reparameterise(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = std.data.new(std.size()).normal_()\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def forward(self, x):\n",
    "        #y = x.view(-1, 784)\n",
    "        #print(y)\n",
    "        mu_logvar = self.encoder(x.view(-1, 12)).view(-1, 2, d)\n",
    "        mu = mu_logvar[:, 0, :]\n",
    "        logvar = mu_logvar[:, 1, :]\n",
    "        z = self.reparameterise(mu, logvar)\n",
    "        return self.decoder(z), mu, logvar\n",
    "\n",
    "model = VAE().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the optimiser\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "\n",
    "def loss_function(x_hat, x, mu, logvar):\n",
    "    BCE = nn.functional.binary_cross_entropy(\n",
    "        x_hat, x.view(-1, 12), reduction='sum'\n",
    "    )\n",
    "    KLD = 0.5 * torch.sum(logvar.exp() - logvar - 1 + mu.pow(2))\n",
    "\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "====> Test set loss: 8.5625\n",
      "====> Epoch: 1 Average loss: 8.6033\n",
      "====> Test set loss: 8.5585\n",
      "====> Epoch: 2 Average loss: 8.5434\n",
      "====> Test set loss: 8.5545\n",
      "====> Epoch: 3 Average loss: 8.5357\n",
      "====> Test set loss: 8.5505\n",
      "====> Epoch: 4 Average loss: 8.5755\n",
      "====> Test set loss: 8.5466\n",
      "====> Epoch: 5 Average loss: 8.5468\n",
      "====> Test set loss: 8.5429\n",
      "====> Epoch: 6 Average loss: 8.5309\n",
      "====> Test set loss: 8.5393\n",
      "====> Epoch: 7 Average loss: 8.5334\n",
      "====> Test set loss: 8.5358\n",
      "====> Epoch: 8 Average loss: 8.5522\n",
      "====> Test set loss: 8.5326\n",
      "====> Epoch: 9 Average loss: 8.5070\n",
      "====> Test set loss: 8.5294\n",
      "====> Epoch: 10 Average loss: 8.5177\n",
      "====> Test set loss: 8.5263\n"
     ]
    }
   ],
   "source": [
    "# Training and testing the VAE\n",
    "\n",
    "epochs = 10\n",
    "codes = dict(μ=list(), logσ2=list(), y=list())\n",
    "for epoch in range(0, epochs + 1):\n",
    "    # Training\n",
    "    if epoch > 0:  # test untrained net first\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for data  in train_loader:\n",
    "            x = data['x']\n",
    "            y = data['y']\n",
    "\n",
    "            x = x.to(device)\n",
    "            # ===================forward=====================\n",
    "            x_hat, mu, logvar = model(x)\n",
    "            loss = loss_function(x_hat, x, mu, logvar)\n",
    "            train_loss += loss.item()\n",
    "            # ===================backward====================\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # ===================log========================\n",
    "        print(f'====> Epoch: {epoch} Average loss: {train_loss / len(train_loader.dataset):.4f}')\n",
    "    \n",
    "    # Testing\n",
    "    \n",
    "    means, logvars, labels = list(), list(), list()\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        for data in test_loader:\n",
    "            x = data['x']\n",
    "            y = data['y']\n",
    "\n",
    "            x = x.to(device)\n",
    "            # ===================forward=====================\n",
    "            x_hat, mu, logvar = model(x)\n",
    "            test_loss += loss_function(x_hat, x, mu, logvar).item()\n",
    "            # =====================log=======================\n",
    "            means.append(mu.detach())\n",
    "            logvars.append(logvar.detach())\n",
    "            labels.append(y.detach())\n",
    "    # ===================log========================\n",
    "    codes['μ'].append(torch.cat(means))\n",
    "    codes['logσ2'].append(torch.cat(logvars))\n",
    "    codes['y'].append(torch.cat(labels))\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'====> Test set loss: {test_loss:.4f}')\n",
    "    #display_images(x, x_hat, 1, f'Epoch {epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a few samples\n",
    "\n",
    "N = 16\n",
    "z = torch.randn((N, d)).to(device)\n",
    "sample = model.decoder(z)\n",
    "#display_images(None, sample, N // 4, count=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display last test batch\n",
    "\n",
    "display_images(None, x, 4, count=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose starting and ending point for the interpolation -> shows original and reconstructed\n",
    "\n",
    "A, B = 1, 14\n",
    "sample = model.decoder(torch.stack((mu[A].data, mu[B].data), 0))\n",
    "display_images(None, torch.stack(((\n",
    "    x[A].data.view(-1),\n",
    "    x[B].data.view(-1),\n",
    "    sample.data[0],\n",
    "    sample.data[1]\n",
    ")), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform an interpolation between input A and B, in N steps\n",
    "\n",
    "N = 16\n",
    "code = torch.Tensor(N, 20).to(device)\n",
    "sample = torch.Tensor(N, 28, 28).to(device)\n",
    "for i in range(N):\n",
    "    code[i] = i / (N - 1) * mu[B].data + (1 - i / (N - 1) ) * mu[A].data\n",
    "    # sample[i] = i / (N - 1) * x[B].data + (1 - i / (N - 1) ) * x[A].data\n",
    "sample = model.decoder(code)\n",
    "display_images(None, sample, N // 4, count=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from res.plot_lib import set_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_default(figsize=(15, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, E = list(), list(), list()  # input, classes, embeddings\n",
    "N = 1000  # samples per epoch\n",
    "epochs = (0, 5, 10)\n",
    "for epoch in epochs:\n",
    "    X.append(codes['μ'][epoch][:N])\n",
    "    E.append(TSNE(n_components=2).fit_transform(X[-1]))\n",
    "    Y.append(codes['y'][epoch][:N])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, a = plt.subplots(ncols=3)\n",
    "for i, e in enumerate(epochs):\n",
    "    s = a[i].scatter(E[i][:,0], E[i][:,1], c=Y[i], cmap='tab10')\n",
    "    a[i].grid(False)\n",
    "    a[i].set_title(f'Epoch {e}')\n",
    "    a[i].axis('equal')\n",
    "f.colorbar(s, ax=a[:], ticks=np.arange(10), boundaries=np.arange(11) - .5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3710jvsc74a57bd06be493098a7402738a5f502b846eb8b183bc25f6cfce641bf76783fe534f68a3",
   "display_name": "Python 3.7.10 64-bit ('mytf2': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "6be493098a7402738a5f502b846eb8b183bc25f6cfce641bf76783fe534f68a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}